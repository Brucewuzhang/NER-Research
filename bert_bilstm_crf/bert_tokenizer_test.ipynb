{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd11eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61718490",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f2b2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40bdcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9687998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replace', 'me', 'by', 'any', 'text', 'you', \"'\", 'd', 'like', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ebde43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BertTokenizer in module transformers.models.bert.tokenization_bert object:\n",
      "\n",
      "class BertTokenizer(transformers.tokenization_utils.PreTrainedTokenizer)\n",
      " |  Construct a BERT tokenizer. Based on WordPiece.\n",
      " |  \n",
      " |  This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
      " |  Users should refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (:obj:`str`):\n",
      " |          File containing the vocabulary.\n",
      " |      do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to lowercase the input when tokenizing.\n",
      " |      do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to do basic tokenization before WordPiece.\n",
      " |      never_split (:obj:`Iterable`, `optional`):\n",
      " |          Collection of tokens which will never be split during tokenization. Only has an effect when\n",
      " |          :obj:`do_basic_tokenize=True`\n",
      " |      unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n",
      " |          The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      " |          sequence classification or for a text and a question for question answering. It is also used as the last\n",
      " |          token of a sequence built with special tokens.\n",
      " |      pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n",
      " |          The token used for padding, for example when batching sequences of different lengths.\n",
      " |      cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n",
      " |          The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      " |          instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      " |      mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n",
      " |          The token used for masking values. This is the token used when training this model with masked language\n",
      " |          modeling. This is the token which the model will try to predict.\n",
      " |      tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |          Whether or not to tokenize Chinese characters.\n",
      " |  \n",
      " |          This should likely be deactivated for Japanese (see this `issue\n",
      " |          <https://github.com/huggingface/transformers/issues/328>`__).\n",
      " |      strip_accents: (:obj:`bool`, `optional`):\n",
      " |          Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
      " |          value for :obj:`lowercase` (as in the original BERT).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertTokenizer\n",
      " |      transformers.tokenization_utils.PreTrainedTokenizer\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.file_utils.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None, unk_token='[UNK]', sep_token='[SEP]', pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', tokenize_chinese_chars=True, strip_accents=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0:List[int], token_ids_1:Union[List[int], NoneType]=None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens. A BERT sequence has the following format:\n",
      " |      \n",
      " |      - single sequence: ``[CLS] X [SEP]``\n",
      " |      - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs to which the special tokens will be added.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens)\n",
      " |      Converts a sequence of tokens (string) in a single string.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0:List[int], token_ids_1:Union[List[int], NoneType]=None) -> List[int]\n",
      " |      Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\n",
      " |      pair mask has the following format:\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " |          | first sequence    | second sequence |\n",
      " |      \n",
      " |      If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
      " |          sequence(s).\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0:List[int], token_ids_1:Union[List[int], NoneType]=None, already_has_special_tokens:bool=False) -> List[int]\n",
      " |      Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer ``prepare_for_model`` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (:obj:`List[int]`):\n",
      " |              List of IDs.\n",
      " |          token_ids_1 (:obj:`List[int]`, `optional`):\n",
      " |              Optional second list of IDs for sequence pairs.\n",
      " |          already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  get_vocab(self)\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      :obj:`tokenizer.get_vocab()[token]` is equivalent to :obj:`tokenizer.convert_tokens_to_ids(token)` when\n",
      " |      :obj:`token` is in the vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory:str, filename_prefix:Union[str, NoneType]=None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      :meth:`~transformers.PreTrainedTokenizerFast._save_pretrained` to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (:obj:`str`, `optional`):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  do_lower_case\n",
      " |  \n",
      " |  vocab_size\n",
      " |      :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  max_model_input_sizes = {'TurkuNLP/bert-base-finnish-cased-v1': 512, '...\n",
      " |  \n",
      " |  pretrained_init_configuration = {'TurkuNLP/bert-base-finnish-cased-v1'...\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {'vocab_file': {'TurkuNLP/bert-base-finni...\n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': 'vocab.txt'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids:Union[int, List[int]], skip_special_tokens:bool=False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`int` or :obj:`List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str` or :obj:`List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens:Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair:bool=False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not\n",
      " |          put this inside your training loop.\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  prepare_for_tokenization(self, text:str, is_split_into_words:bool=False, **kwargs) -> Tuple[str, Dict[str, Any]]\n",
      " |      Performs any necessary transformations before tokenization.\n",
      " |      \n",
      " |      This method should pop the arguments from kwargs and return the remaining :obj:`kwargs` as well. We test the\n",
      " |      :obj:`kwargs` at the end of the encoding process to be sure all the arguments have been used.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The text to prepare.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          kwargs:\n",
      " |              Keyword arguments to use for the tokenization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.\n",
      " |  \n",
      " |  tokenize(self, text:str, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, using the tokenizer.\n",
      " |      \n",
      " |      Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies\n",
      " |      (BPE/SentencePieces/WordPieces). Takes care of added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          **kwargs (additional keyword arguments):\n",
      " |              Passed along to the model-specific ``prepare_for_tokenization`` preprocessing method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils.PreTrainedTokenizer:\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text:Union[str, List[str], List[List[str]]], text_pair:Union[str, List[str], List[List[str]], NoneType]=None, add_special_tokens:bool=True, padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, is_split_into_words:bool=False, pad_to_multiple_of:Union[int, NoneType]=None, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, return_token_type_ids:Union[bool, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_overflowing_tokens:bool=False, return_special_tokens_mask:bool=False, return_offsets_mapping:bool=False, return_length:bool=False, verbose:bool=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              :obj:`is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences:Union[List[int], List[List[int]], _ForwardRef('np.ndarray'), _ForwardRef('torch.Tensor'), _ForwardRef('tf.Tensor')], skip_special_tokens:bool=False, clean_up_tokenization_spaces:bool=True, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (:obj:`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs:Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens:bool=True, padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, is_split_into_words:bool=False, pad_to_multiple_of:Union[int, NoneType]=None, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, return_token_type_ids:Union[bool, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_overflowing_tokens:bool=False, return_special_tokens_mask:bool=False, return_offsets_mapping:bool=False, return_length:bool=False, verbose:bool=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (:obj:`List[str]`, :obj:`List[Tuple[str, str]]`, :obj:`List[List[str]]`, :obj:`List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also :obj:`List[List[int]]`, :obj:`List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in ``encode_plus``).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  decode(self, token_ids:Union[int, List[int], _ForwardRef('np.ndarray'), _ForwardRef('torch.Tensor'), _ForwardRef('tf.Tensor')], skip_special_tokens:bool=False, clean_up_tokenization_spaces:bool=True, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (:obj:`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the ``__call__`` method.\n",
      " |          skip_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to clean up the tokenization spaces.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text:Union[str, List[str], List[int]], text_pair:Union[str, List[str], List[int], NoneType]=None, add_special_tokens:bool=True, padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`List[int]`, :obj:`torch.Tensor`, :obj:`tf.Tensor` or :obj:`np.ndarray`: The tokenized ids of the\n",
      " |          text.\n",
      " |  \n",
      " |  encode_plus(self, text:Union[str, List[str], List[int]], text_pair:Union[str, List[str], List[int], NoneType]=None, add_special_tokens:bool=True, padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, is_split_into_words:bool=False, pad_to_multiple_of:Union[int, NoneType]=None, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, return_token_type_ids:Union[bool, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_overflowing_tokens:bool=False, return_special_tokens_mask:bool=False, return_offsets_mapping:bool=False, return_length:bool=False, verbose:bool=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is deprecated, ``__call__`` should be used instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n",
      " |              method).\n",
      " |          text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the ``tokenize`` method) or a list of integers (tokenized string ids using the\n",
      " |              ``convert_tokens_to_ids`` method).\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  pad(self, encoded_inputs:Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=True, max_length:Union[int, NoneType]=None, pad_to_multiple_of:Union[int, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, verbose:bool=True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with ``self.padding_side``,\n",
      " |      ``self.pad_token_id`` and ``self.pad_token_type_id``)\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``encoded_inputs`` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |          result will use the same type unless you provide a different tensor type with ``return_tensors``. In the\n",
      " |          case of PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs (:class:`~transformers.BatchEncoding`, list of :class:`~transformers.BatchEncoding`, :obj:`Dict[str, List[int]]`, :obj:`Dict[str, List[List[int]]` or :obj:`List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input (:class:`~transformers.BatchEncoding` or :obj:`Dict[str,\n",
      " |              List[int]]`) or a batch of tokenized inputs (list of :class:`~transformers.BatchEncoding`, `Dict[str,\n",
      " |              List[List[int]]]` or `List[Dict[str, List[int]]]`) so you can use this method during preprocessing as\n",
      " |              well as in a PyTorch Dataloader collate function.\n",
      " |      \n",
      " |              Instead of :obj:`List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n",
      " |              see the note above for the return type.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              >= 7.5 (Volta).\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids:List[int], pair_ids:Union[List[int], NoneType]=None, add_special_tokens:bool=True, padding:Union[bool, str, transformers.file_utils.PaddingStrategy]=False, truncation:Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]=False, max_length:Union[int, NoneType]=None, stride:int=0, pad_to_multiple_of:Union[int, NoneType]=None, return_tensors:Union[str, transformers.file_utils.TensorType, NoneType]=None, return_token_type_ids:Union[bool, NoneType]=None, return_attention_mask:Union[bool, NoneType]=None, return_overflowing_tokens:bool=False, return_special_tokens_mask:bool=False, return_offsets_mapping:bool=False, return_length:bool=False, verbose:bool=True, prepend_batch_axis:bool=False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |      \n",
      " |          add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to encode the sequences with the special tokens relative to their model.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum\n",
      " |              length is required by one of the truncation/padding parameters. If the model has no specific maximum\n",
      " |              input length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a number along with :obj:`max_length`, the overflowing tokens returned when\n",
      " |              :obj:`return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to :obj:`True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (:obj:`int`, `optional`):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (:obj:`bool`, `optional`):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |          return_attention_mask (:obj:`bool`, `optional`):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return overflowing token sequences.\n",
      " |          return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return :obj:`(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from\n",
      " |              :class:`~transformers.PreTrainedTokenizerFast`, if using Python's tokenizer, this method will raise\n",
      " |              :obj:`NotImplementedError`.\n",
      " |          return_length  (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the :obj:`self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when :obj:`return_token_type_ids=True`\n",
      " |            or if `\"token_type_ids\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are token type IDs? <../glossary.html#token-type-ids>`__\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            :obj:`return_attention_mask=True` or if `\"attention_mask\"` is in :obj:`self.model_input_names`).\n",
      " |      \n",
      " |            `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a :obj:`max_length` is specified and\n",
      " |            :obj:`return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when :obj:`add_special_tokens=True` and :obj:`return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when :obj:`return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts:List[str], tgt_texts:Union[List[str], NoneType]=None, max_length:Union[int, NoneType]=None, max_target_length:Union[int, NoneType]=None, padding:str='longest', return_tensors:str=None, truncation:bool=True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (:obj:`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (:obj:`list`, `optional`):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to :obj:`None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (:obj:`int`, `optional`):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to :obj:`None`, this will use the max_length value.\n",
      " |          padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n",
      " |                single sequence if provided).\n",
      " |              * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
      " |                different lengths).\n",
      " |          return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      " |              * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      " |              * :obj:`'np'`: Return Numpy :obj:`np.ndarray` objects.\n",
      " |          truncation (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              * :obj:`True` or :obj:`'longest_first'`: Truncate to a maximum length specified with the argument\n",
      " |                :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not\n",
      " |                provided. This will truncate token by token, removing a token from the longest sequence in the pair\n",
      " |                if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`False` or :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with\n",
      " |                sequence lengths greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to :obj:`self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.BatchEncoding`: A :class:`~transformers.BatchEncoding` with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys ``[input_ids, attention_mask, labels]``, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  save_pretrained(self, save_directory:Union[str, os.PathLike], legacy_format:Union[bool, NoneType]=None, filename_prefix:Union[str, NoneType]=None, push_to_hub:bool=False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizer.from_pretrained` class method..\n",
      " |      \n",
      " |      .. Warning::\n",
      " |         This won't save modifications you may have applied to the tokenizer after the instantiation (for instance,\n",
      " |         modifying :obj:`tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (:obj:`str` or :obj:`os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (:obj:`bool`, `optional`):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format, i.e. with tokenizer specific vocabulary and a separate added_tokens\n",
      " |              files.\n",
      " |      \n",
      " |              If :obj:`False`, will only save the tokenizer in the unified JSON format. This format is incompatible\n",
      " |              with \"slow\" tokenizers (not powered by the `tokenizers` library), so the tokenizer will not be able to\n",
      " |              be loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If :obj:`True`, will save the tokenizer in legacy format.\n",
      " |          filename_prefix: (:obj:`str`, `optional`):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of :obj:`str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids:List[int], pair_ids:Union[List[int], NoneType]=None, num_tokens_to_remove:int=0, truncation_strategy:Union[str, transformers.tokenization_utils_base.TruncationStrategy]='longest_first', stride:int=0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (:obj:`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          pair_ids (:obj:`List[int]`, `optional`):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the ``tokenize``\n",
      " |              and ``convert_tokens_to_ids`` methods.\n",
      " |          num_tokens_to_remove (:obj:`int`, `optional`, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (:obj:`str` or :class:`~transformers.tokenization_utils_base.TruncationStrategy`, `optional`, defaults to :obj:`False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              * :obj:`'longest_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_first'`: Truncate to a maximum length specified with the argument :obj:`max_length` or to\n",
      " |                the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'only_second'`: Truncate to a maximum length specified with the argument :obj:`max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              * :obj:`'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          stride (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`Tuple[List[int], List[int], List[int]]`: The truncated ``ids``, the truncated ``pair_ids`` and the\n",
      " |          list of overflowing tokens.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path:Union[str, os.PathLike], *init_inputs, **kwargs) from builtins.type\n",
      " |      Instantiate a :class:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase` (or a derived class) from\n",
      " |      a predefined tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the `model id` of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under a\n",
      " |                user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      " |              - A path to a `directory` containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the :meth:`~transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`\n",
      " |                method, e.g., ``./my_model_directory/``.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                ``./my_model_directory/vocab.txt``.\n",
      " |          cache_dir (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (:obj:`Dict[str, str], `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      " |          revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (:obj:`str`, `optional`):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, `optional`):\n",
      " |              Will be passed along to the Tokenizer ``__init__`` method.\n",
      " |          kwargs (additional keyword arguments, `optional`):\n",
      " |              Will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like\n",
      " |              ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``,\n",
      " |              ``mask_token``, ``additional_special_tokens``. See parameters in the ``__init__`` for more details.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # We can't instantiate directly the base class `PreTrainedTokenizerBase` so let's show our examples on a derived class: BertTokenizer\n",
      " |          # Download vocabulary from huggingface.co and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |          tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
      " |      \n",
      " |          # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
      " |      \n",
      " |          # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |          tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
      " |      \n",
      " |          # You can link tokens to special vocabulary when instantiating\n",
      " |          tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
      " |          # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |          # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |          assert tokenizer.unk_token == '<unk>'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string:str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (:obj:`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      :obj:`int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      :obj:`int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __annotations__ = {'max_model_input_sizes': typing.Dict[str, typing.Un...\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'token_type_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  slow_tokenizer_class = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict:Dict[str, Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Using :obj:`add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split).\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like :obj:`tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      :class:`~transformers.BertTokenizer` :obj:`cls_token` is already registered to be :obj`'[CLS]'` and XLM's one\n",
      " |      is also registered to be :obj:`'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary `str` to `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``,\n",
      " |              ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
      " |              ``additional_special_tokens``].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the ``unk_token`` to them).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to add a new classification token to GPT-2\n",
      " |          tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
      " |          model = GPT2Model.from_pretrained('gpt2')\n",
      " |      \n",
      " |          special_tokens_dict = {'cls_token': '<CLS>'}\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |          # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |          assert tokenizer.cls_token == '<CLS>'\n",
      " |  \n",
      " |  add_tokens(self, new_tokens:Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens:bool=False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary.\n",
      " |      \n",
      " |      .. Note::\n",
      " |          When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of\n",
      " |          the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |          In order to do that, please use the :meth:`~transformers.PreTrainedModel.resize_token_embeddings` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (:obj:`str`, :obj:`tokenizers.AddedToken` or a list of `str` or :obj:`tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. :obj:`tokenizers.AddedToken` wraps a\n",
      " |              string token to let you personalize its behavior: whether this token should only match against a single\n",
      " |              word, whether this token should strip all potential whitespaces on the left side, whether this token\n",
      " |              should strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for :obj:`tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |          tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      " |          model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
      " |          print('We have added', num_added_toks, 'tokens')\n",
      " |           # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |          model.resize_token_embeddings(len(tokenizer))\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      Make sure that all the special tokens attributes of the tokenizer (:obj:`tokenizer.mask_token`,\n",
      " |      :obj:`tokenizer.cls_token`, etc.) are in the vocabulary.\n",
      " |      \n",
      " |      Add the missing ones to the vocabulary if needed.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`int`: The number of tokens added in the vocabulary during the operation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      :obj:`List[str]`: All the additional special tokens you may want to use. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      :obj:`List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      :obj:`List[int]`: List the ids of the special tokens(:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class\n",
      " |      attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      :obj:`List[str]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.) mapped to class attributes.\n",
      " |      \n",
      " |      Convert tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      :obj:`List[Union[str, tokenizers.AddedToken]]`: All the special tokens (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.)\n",
      " |      mapped to class attributes.\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  bos_token\n",
      " |      :obj:`str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns :obj:`None` if the token\n",
      " |      has not been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      :obj:`str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the\n",
      " |      full depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      :obj:`Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input\n",
      " |      sequence leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      :obj:`str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      :obj:`Optional[int]`: Id of the end of sentence token in the vocabulary. Returns :obj:`None` if the token has\n",
      " |      not been set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      :obj:`str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      :obj:`Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      :obj:`str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      :obj:`Optional[int]`: Id of the padding token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      :obj:`int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  sep_token\n",
      " |      :obj:`str`: Separation token, to separate context and query in an input sequence. Log an error if used while\n",
      " |      not having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      :obj:`Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns :obj:`None` if the token has not been set.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      :obj:`Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (:obj:`cls_token`,\n",
      " |      :obj:`unk_token`, etc.) to their values (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of :obj:`tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      :obj:`Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary\n",
      " |      mapping special token class attributes (:obj:`cls_token`, :obj:`unk_token`, etc.) to their values\n",
      " |      (:obj:`'<unk>'`, :obj:`'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of :obj:`tokenizers.AddedToken` type to string so they can be used to control more finely\n",
      " |      how special tokens are tokenized.\n",
      " |  \n",
      " |  unk_token\n",
      " |      :obj:`str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      :obj:`Optional[int]`: Id of the unknown token in the vocabulary. Returns :obj:`None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.file_utils.PushToHubMixin:\n",
      " |  \n",
      " |  push_to_hub(self, repo_name:Union[str, NoneType]=None, repo_url:Union[str, NoneType]=None, commit_message:Union[str, NoneType]=None, organization:Union[str, NoneType]=None, private:bool=None, use_auth_token:Union[bool, str, NoneType]=None) -> str\n",
      " |      Upload model checkpoint or tokenizer files to the  model hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_name (:obj:`str`, `optional`):\n",
      " |              Repository name for your model or tokenizer in the hub. If not specified, the repository name will be\n",
      " |              the stem of :obj:`save_directory`.\n",
      " |          repo_url (:obj:`str`, `optional`):\n",
      " |              Specify this in case you want to push to an existing repository in the hub. If unspecified, a new\n",
      " |              repository will be created in your namespace (unless you specify an :obj:`organization`) with\n",
      " |              :obj:`repo_name`.\n",
      " |          commit_message (:obj:`str`, `optional`):\n",
      " |              Message to commit while pushing. Will default to :obj:`\"add config\"`, :obj:`\"add tokenizer\"` or\n",
      " |              :obj:`\"add model\"` depending on the type of the class.\n",
      " |          organization (:obj:`str`, `optional`):\n",
      " |              Organization in which you want to push your model or tokenizer (you must be a member of this\n",
      " |              organization).\n",
      " |          private (:obj:`bool`, `optional`):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (:obj:`bool` or :obj:`str`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`). Will default to\n",
      " |              :obj:`True` if :obj:`repo_url` is not specified.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          The url of the commit of your model in the given repository.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171e6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"replace me by any text you'd like .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c0b1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=\n",
       "array([[ 101, 4971, 1143, 1118, 1251, 3087, 1128,  112,  173, 1176,  119,\n",
       "         102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 12), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d932104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
